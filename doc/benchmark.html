<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <link href="style.css" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta content="text/html; charset=ISO-8859-1" http-equiv="content-type">
    <title>Benchmarking backup tools</title>
  </head>
  <body>

    <center><h1>Benchmarking backup tools</h1></center>

    <h2>Introduction</h2>
    <p>
      This document has for objective to compare <b>dar</b> to other common backup tools under Unix
      (Linux, FreeBSD, MACOS X...), among
      the most commonly available today. The <b>first target</b> is being able to restore a whole system
      from a minimal environment without assistance of an already existing local server (disaster context).
      The <b>second target</b> evaluating the security of archived data, when time passes.
    </p>
    <p>
      For that reason we have excluded complex softwares that requires servers already running
      on the local network (<i>Bacula</i>, <i>Amanda</i>, <i>Bareos</i>, <i>UrBackup</i>, <i>Burp</i>...) assuming you have first to reconstruct such server in
      case of disaster in order to be able to use this type of software backup. We have also filtered out softwares
      that only provide a Graphical User Interface (GUI) to still be able to face situation
      where even graphical environment is not available.
    </p>
    <p>
      We also had to exclude partition cloning systems (<i>clonezilla</i>, <i>MondoRescue</i>, <i>RescueZilla</i>,
      <i>partclone</i> and consorts) as they are targetted at block copy and as such cannot backup a live system:
      you have to shutdown and boot on a CD/USB key to proceed or run in single user-mode. This cannot
      be automated and is impacting for the user as he has to interrupt his work and perform the operation by
      hand, wait for its completion to be able to use back again the system.
    <p>
      The comparison will thus concerns the following tools:
    </p>
    <ul>
      <li><b>dar</b> and software based on it: gdar, DarGUI, Baras, Darbup, Darbrrd, HUbackup, SaraB...</li>
      <li><b>rsync</b> and software based on it: TimeShift, rsnapshot... </li>
      <li><b>tar</b> and software based on it: BackupPC, Duplicity, fwbackups... </li>

    </ul>

    <h2>Tests Famillies</h2>
    <p>
      Several aspects are interesting to consider:
    </p>
    <h3>For Backups</h3>
    <lu>
      <li><b>completness</b> of the restoration: file permissions, dates precision, hardlinks, file attributes, Extended Attributes, sparse files...</li>
      <li><b>execution performance</b>: execution time, memory consumption, multi-threading support...</li>
      <li><b>disk space efficiency</b>: this mainly depend on the compression algorithm used so we will chose one and compare with that one</li>
      <li><b>robustness</b> of the backup: data corruption impact</li>
      <li>main <b>features</b> around backup: differential backup, snapshot, deduplication, file's history...</li>
      <li><b>cloud storage compatibility</b>: data ciphering, network protocol suported...</li>
      <li><b>ease of restoration</b> of a whole system</li>
    </lu>

    <h3>For Archiving</h3>
    <p>
      For the second target (archiving) a subset of the previous points are interesting but with
      more emphasis and a different context:
    </p>
    <lu>
      <li><b>disk space efficiency</b></li>
      <li><b>robustness</b> of the archive</li>
      <li><b>ease of restoration</b> of archived data: speed and operation</li>
    </lu>

    <h2>Testing Methodology</h2>

    <p>
      We will not work on random data, but on an adapted sets of data to stress the most crutial aspect of each test:
    </p>
    <ul>
      <li>to measure <b>completness</b> we will generate a small set of file with the script defined below</li>
      <li>to measure <b>performance</b> we will use first two data sets:
	<ul>
	  <li>A single large ISO file</li>
	  <li>The linux kernel source tree (kernel 5.9.2) which contains a lot of small files</li>
	</ul>
    </ul>
    <p>
      When measuring time performance and not to measure disk (low) performances,
      we will use <b>tmpfs</b> filesystem for the data to backup and for the place to store the resulting backup to.
    </p>

    <p>
      The following script (also available with dar source code) is used to build a
      directory tree to evaluate the completness of the backup tools.
    </p>
    <div><code>
	#!/bin/bash

	if [ -z "$1" ] ; then
	echo "usage: $0 &lt;directory&gt;"
	exit 1
	fi

	if [ -e "$1" ] ; then
	echo "$1 already exists, remove it or use another directory name"
	exit 1
	fi

	if ! dar -V &gt; /dev/null ; then
	echo "need dar to copy unix socket to the test tree"
	exit 1
	fi

	mkdir "$1"
	cd "$1"

	# creating
	mkdir "SUB"
	dd if=/dev/zero of=plain_zeroed bs=1024 count=1024
	dd if=/dev/urandom of=random bs=1024 count=1024
	dd if=/dev/zero of=sparse_file bs=1 count=1 seek=10239999
	ln -s random SUB/symlink-broken
	ln -s ../random SUB/symlink-valid
	mkfifo pipe
	mknod null c 3 1
	mknod fd1 b 2 1
	dar -c - -R / -g dev/log -N -Q -q | dar -x - --sequential-read -N -q -Q
	ln sparse_file SUB/hard_linked_sparse_file
	ln dev/log SUB/hard_linked_socket
	ln pipe SUB/hard_linked_pipe

	# modifying dates and permissions
	sleep 2
	chown nobody random
	chown -h bin SUB/symlink-valid
	chgrp -h daemon SUB/symlink-valid
	sleep 2
	echo hello &gt;&gt; random
	sleep 2
	cat &lt; random &gt; /dev/null

	# adding Extend Attributes, assuming the filesystem as user_xattr and acl option set
	setfacl -m u:nobody:rwx plain_zeroed && setfattr -n "user.hello" -v "hello world!!!" plain_zeroed || (echo "FAILED TO CREATE EXTENDED ATTRIBUTES" && exit 1)

	# adding filesystem specific attributes
	chattr +dis plain_zeroed

    </code></div>

    <p>
      Another tool is used to modify a file to backup or emulate archive corruption. It flips a bit
      an a given offset in a file
    </p>

    <div><code>
	root@devuan:~# cat bitflip
	#!/bin/bash

	if [ -z "$1" -o -z "$2" ] ; then
	  echo "usage: $0 &lt;offset in bit&gt; &lt;file&gt;"
	  echo "flip the bit of the file located at the provided offset"
	  exit 1
	fi

	offbit=$1
	file="$2"

	offbyte=$(( $offbit / 8 ))
	bitinbyte=$(( $offbit - ($offbyte * 8) ))
	readbyte=`xxd -s $offbyte -p -l 1 "$file"`
	mask=$(( 1 &lt;&lt; $bitinbyte ))
	newbyte=$(( 0x$readbyte ^ $mask ))
	hexanewbyte=`printf "%.2x" $newbyte`

	echo $hexanewbyte | xxd -p -l 1 -s $offbyte -r - "$file"
	root@devuan:~#
    </code></div>

    <p>
      <b><u>Testing hardware</u></b>
    </p>

    <p>
      Tests results that follow come from the execution inside a proxmox VE virtual machine, running a Devuan beowulf system on a Intel Core i5-7400 (3 GHz) processor.
      The VM is allocated 12 GiB or RAM and the 4 CPU cores of the system. Testing on baremetal would give better performances, but comparison stay interesting.
    </p>

    <h2>Test Results</h2>

    <h3>Completness of backup and restoration</h3>

    <table border="0" style="text-align: center;">
      <tr>
	<td class="title">Software</td>
	<td class="title">plain file</td>
	<td class="title">symlink</td>
	<td class="title">hardlinked files</td>
	<td class="title">hardlinked sockets</td>
	<td class="title">hardlinked pipes</td>
	<td class="title">user</td>
	<td class="title">group</td>
	<td class="title">perm.</td>
	<td class="title">ACL</td>
	<td class="title">Extended Attributes</td>
	<td class="title">FS Attributes</td>
	<td class="title">atime</td>
	<td class="title">mtime</td>
	<td class="title">ctime</td>
	<td class="title">btime</td>
	<td class="title">Spares File</td>
	<td class="title">Disk usage optimization</td>
      </tr>
      <tr>
	<td>Dar</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>-</td>
	<td>yes(1)</td>
	<td>yes</td>
	<td>yes</td>
      </tr>
      <tr>
	<td>Rsync</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>-</td>
	<td>-</td>
	<td>yes</td>
	<td>-</td>
	<td>yes(1)</td>
	<td>-</td>
	<td>-</td>
      </tr>
      <tr>
	<td>Tar</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>-</td>
	<td>-</td>
	<td>yes</td>
	<td>yes</td>
	<td>yes</td>
	<td>-</td>
	<td>-</td>
	<td>-</td>
	<td>-</td>
	<td>yes(3)</td>
	<td>-</td>
	<td>yes(1)</td>
	<td>-</td>
	<td>-</td>
      </tr>
    </table>
    <ul>
      <li>(1) under MACoS X, FreeBSD and BSD systems. As of today (year 2020), Linux has not way to set the <i>btime</i> aka birthtime or creation time</li>
      <li>(2) <i>tar</i> does even not save plain normal sockets</li>
      <li>(3) <i>mtime</i> is saved by <i>tar</i> but with an accuracy of only 1 second, while today's systems provide nanosecond precision</li>
    </p>
    <h4>Sparse File impact</h4>
    <p>
      One could argue that sparse file is never used, this is wrong! <code>/var/log/lastlog</code>, <code>/var/log/faillog</code>
      are two examples of sparse file that <b>any</b> Unix filesystem has. Depending on the installed applications, your system may
      have virtually huge sparse files that would not be a good idea to save as plain file (you could fail restoring your data even onto a larger
      disk).
    </p>
    <h4>Space optimization</h4>
    <p>
      What the impact of space optimization brought my <i>dar</i> at restoration time? Let's take an example with <code>/usr/bin</code> directory under Linux:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# rm -rf DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du -s /usr/bin</b>
	<e>431132</e>  /usr/bin
	<b>root@terre:/mnt/localdisk/Benchmark_tools# dar -c backup -R /usr/bin -q</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# mkdir DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# dar -x backup -R DST</b>


	--------------------------------------------
	2593 inode(s) restored
	including 5 hard link(s)
	0 inode(s) not restored (not saved in archive)
	0 inode(s) not restored (overwriting policy decision)
	0 inode(s) ignored (excluded by filters)
	0 inode(s) failed to restore (filesystem error)
	0 inode(s) deleted
	--------------------------------------------
	Total number of inode(s) considered: 2593
	--------------------------------------------
	EA restored for 3 inode(s)
	FSA restored for 0 inode(s)
	--------------------------------------------
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du -s DST</b>
	<e>398144</e>  DST
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>

    </code></div>
    <p>
      Here, we get a disk space saving of <b>7.6 %</b>, this also means faster
      file reading operation and better disk life preservation in the same ratio (less disk I/O),
      and of course you get 7.6 % more disk space available to store some other data.
    </p>

    <h3>Execution Performance</h3>
    <p>
      Several tests have been used to evuate performance
    </p>

    <h4>Full backup</h4>
    <p>
    <p>
      Full backup without compression has some interest when execution time
      is important and operation is of short delay. The disk space used is
      not much of a concern, the CPU and memory requirement. Typical context
      is a copy of a directory tree locally or remotely over a large bandwidth
      network.
    </p>
    <p>
      In this context, restoration is usually full and systematically executed right
      after the backup, which is then destroyed.
    </p>
    <p>
      Full backup with compression has space requirement as first objective, this usually
      matches the relative long term storage (compared to the backup operation itself).
      In case of remote backup (or remote copy) compression is also interesting with low
      bandwidth networks, when the compression time overhead saved more time than
      time to transfer the volume of data gained by the compression.
    </p>
    <p>
      In this context, the restoration not used often, and may be used several times
      to restore a particular file that has been accidentally destroyed. Though the
      disaster situation may arise and the full restoration is then applied.
    </p>
    <p>
      With this in mind have performed several tests for all software under test:
    </p>
    <ul>
      <li>Full backup without compression</li>
      <li>Full Restoration without compression</li>
      <li>Restoration of a single file without compression</li>
      <li>Full backup with gzip compression (single thread)</li>
      <li>Full backup with gzip compression (several threads)</li>
      <li>Full Restoration with gzip compression (single thread)</li>
      <li>Full Restoration with gzip compression (multiple threads)</li>
      <li>Restoration of a single file with compression</li>
    </ul>

    <p>
      This operation has been performed on different data sets:
    </p>
    <ul>
      <li>Debian iso image</li>
      <li>Linux kernel source tree</li>
    </ul>

    <p>
      The tests gave the following results (see test logs below for details):
    </p>


    <h4>Differential Backup</h4>

    <h3>Disk Space Efficiency</h3>

    <h3>Robustness</h3>

    <h3>Feature set</h3>
    history, recover a deleted file, restore a whole system

    <h3>Remote and Cloud storage Compatibility</h3>

    <h3>Ease of restoration</h3>


    <h2>Conclusion</h2>

    <p>
      We could see that there is not a single backup tool that match all need. We could identify
      different use cases for which however a tool is better than the other:
    </p>
    <ul>
      <li>Backup
	<ul>
	  <li>
	    Without backup history
	    <ul>
	      <li>
		Only full backups
		<ul>
		  <li>for user data with no specific filesystem features (EA, filesystem attribute, ACL, ...)
		    <ul>
		      <li>reduced storage backup -> tar (or dar eventually)</li>
		      <li>copy-like backup -> rsync, (or cp if local backup)</li>
		    </ul>
		  </li>
		  <li>for system data rich metadata (EA, filesystem attributes, ACL...)
		    <ul>
		      <li>reduced storage backup -> dar</li>
		      <li>copy-like backup -> rsync</li>
		    </ul>
		  </li>
		  <li>for system data with rich metadata and sparse files -> dar</li>
		</ul>
	      </li>
	      <li>
		Differential backup
		<ul>
		  <li>for data with poor metadates (EA, filesystem attribute, ACL, ...) -> rsync, dar
		    <ul>
		      <li>reduced storage backup -> dar, tar</li>
		      <li>copy-like backup -> rsync</li>
		    </ul>
		  </li>
		  <li>for data with rich metadata (EA, filesystem attributes, ACL...)
		    <ul>
		      <li>reduced storage backup -> dar</li>
		      <li>copy-like backup -> rsync, dar</li>
		    </ul>
		  </li>
		  <li>for data with rich metadata and spares files -> dar</li>
		</ul>
	      </li>
	      <li>
		more complex backup rotation scheme
		<ul>
		  <li>for user data with no specific filesystem features (EA, filesystem attribute, ACL, ...) -> dar, tar</li>
		  <li>for data with rich metadata and/or sparse files... -> dar</li>
		</ul>
	    </ul>
	  </li>
	  <li>With backup history
	    <ul>
	      <li>
		Only full backups
		<ul>
		  <li>for user data with no specific filesystem features (EA, filesystem attribute, ACL, ...) -> tar, dar</li>
		  <li>for system data with sparse files, EA, filesystem attributes, ACL...-> dar</li>
		</ul>
	      </li>
	      <li>
		Differential backup and more complex backup rotation scheme
		<ul>
		  <li>for user data with no specific filesystem features (EA, filesystem attribute, ACL, ...) -> dar</li>
		  <li>for system data with sparse files, EA, filesystem attributes, ACL...-> dar</li>
		</ul>
	      </li>
	    </ul>
	  </li>
	</ul>
      </li>
      <li>
	Archiving
	<ul>
	  <li>On low quality media -> dar</li>
	  <li>On high quality media -> tar, dar</li>
	</ul>
      </li>
    </ul>

    <p>
      Depending on your objective, I hope this document gives you a better view of what software best match your need.
    </p>


    <h2>Test logs</h2>
    <h3>Software used for the tests</h3>
    <ul>
      <li><b>dar</b> version 2.7.0</li>
      <li><b>rsync</b> version 3.1.3</li>
      <li><b>tar</b> GNU tar 1.30 under Linux and GNU tar (gdar) 1.32 under FreeBSD</li>
    </ul>

    <h3>Completness of backup and restoration</h3>
    <h4>Dar</h4>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# ./build_test_tree.bash  SRC</b>
	1024+0 records in
	1024+0 records out
	1048576 bytes (1.0 MB, 1.0 MiB) copied, 0.00395381 s, 265 MB/s
	1024+0 records in
	1024+0 records out
	1048576 bytes (1.0 MB, 1.0 MiB) copied, 0.00621889 s, 169 MB/s
	1+0 records in
	1+0 records out
	1 byte copied, 0.000386102 s, 2.6 kB/s
	<b>root@terre:/mnt/localdisk/Benchmark_tools# dar -c backup -R SRC</b>


	--------------------------------------------
	14 inode(s) saved
	including 3 hard link(s) treated
	0 inode(s) changed at the moment of the backup and could not be saved properly
	0 byte(s) have been wasted in the archive to resave changing files
	0 inode(s) with only metadata changed
	0 inode(s) not saved (no inode/file change)
	0 inode(s) failed to be saved (filesystem error)
	0 inode(s) ignored (excluded by filters)
	0 inode(s) recorded as deleted from reference backup
	--------------------------------------------
	Total number of inode(s) considered: 14
	--------------------------------------------
	EA saved for 1 inode(s)
	FSA saved for 5 inode(s)
	--------------------------------------------
	<b>root@terre:/mnt/localdisk/Benchmark_tools# mkdir DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# dar -x backup -R DST</b>


	--------------------------------------------
	14 inode(s) restored
	including 3 hard link(s)
	0 inode(s) not restored (not saved in archive)
	0 inode(s) not restored (overwriting policy decision)
	0 inode(s) ignored (excluded by filters)
	0 inode(s) failed to restore (filesystem error)
	0 inode(s) deleted
	--------------------------------------------
	Total number of inode(s) considered: 14
	--------------------------------------------
	EA restored for 1 inode(s)
	FSA restored for 1 inode(s)
	--------------------------------------------
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      We simply performed backup of <code>SRC</code> directory with dar's default options, then
      restore this backup into the <code>DST</code> directory, let's now compare <code>SRC</code> and <code>DST</code> contents:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk# du -s SRC DST</b>
	2068    SRC
	<e>1048</e>    DST
	<b>root@terre:/mnt/localdisk#</b>
    </code></div>
    <p>
      The space used by <code>DST</code> is less than the space used by <code>SRC</code>! At first
      we could beleive that not all data could be restored, let's looking for the explanation:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk#ls -iRl SRC DST</b>
	DST:
	total 1044
	414844 drwxr-xr-x  2 root   root     4096 Oct 28 11:09 SUB
	414850 drwxr-xr-x  2 root   root     4096 Oct 22 11:09 dev
	414848 brw-r--r--  1 root   root     2, 1 Oct 28 11:09 fd1
	414842 crw-r--r--  1 root   root     3, 1 Oct 28 11:09 null
	<e class="red">414841</e> prw-r--r--  2 root   root        0 Oct 28 11:09 pipe
	414840 -rw-rwxr--+ 1 root   root  1048576 Oct 28 11:09 plain_zeroed
	414849 -rw-r--r--  1 nobody root  1048582 Oct 28 11:09 random
	<e>414843</e> -rw-r--r--  2 root   root 10240000 Oct 28 11:09 sparse_file

	DST/SUB:
	total 4
	<e class="red">414841</e> prw-r--r-- 2 root root          0 Oct 28 11:09 hard_linked_pipe
	<e class="blue">414846</e> srw-rw-rw- 2 root root          0 Oct 12 23:00 hard_linked_socket
	<e>414843</e> -rw-r--r-- 2 root root   10240000 Oct 28 11:09 hard_linked_sparse_file
	414845 lrwxrwxrwx 1 root root          6 Oct 28 11:09 symlink-broken -> random
	414847 lrwxrwxrwx 1 bin  daemon        9 Oct 28 11:09 symlink-valid -> ../random

	DST/dev:
	total 0
	<e class="blue">414846</e> srw-rw-rw- 2 root root 0 Oct 12 23:00 log

	SRC:
	total 2064
	411386 drwxr-xr-x  2 root   root     4096 Oct 28 11:09 SUB
	414836 drwxr-xr-x  2 root   root     4096 Oct 22 11:09 dev
	414835 brw-r--r--  1 root   root     2, 1 Oct 28 11:09 fd1
	414834 crw-r--r--  1 root   root     3, 1 Oct 28 11:09 null
	414832 prw-r--r--  2 root   root        0 Oct 28 11:09 pipe
	414826 -rw-rwxr--+ 1 root   root  1048576 Oct 28 11:09 plain_zeroed
	414827 -rw-r--r--  1 nobody root  1048582 Oct 28 11:09 random
	414828 -rw-r--r--  2 root   root 10240000 Oct 28 11:09 sparse_file

	SRC/SUB:
	total 4
	414832 prw-r--r-- 2 root root          0 Oct 28 11:09 hard_linked_pipe
	414837 srw-rw-rw- 2 root root          0 Oct 12 23:00 hard_linked_socket
	414828 -rw-r--r-- 2 root root   10240000 Oct 28 11:09 hard_linked_sparse_file
	414830 lrwxrwxrwx 1 root root          6 Oct 28 11:09 symlink-broken -> random
	414831 lrwxrwxrwx 1 bin  daemon        9 Oct 28 11:09 symlink-valid -> ../random

	SRC/dev:
	total 0
	414837 srw-rw-rw- 2 root root 0 Oct 12 23:00 log
	<b>root@terre:/mnt/localdisk#</b>
    </code></div>
    <p>
      All files are present in <code>DST</code> and use the expected space usage, as reported by the <code>ls</code> command.
      We can also see that the hard linked inode were properly restored for plain file, named pipe and unix socket: the
      inode number in first column is the same (see colorized output above).
    </p>
    <p>
      Maybe something is missing elsewhere?
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk# getfacl SRC/plain_zeroed DST/plain_zeroed</b>
	# file: SRC/plain_zeroed
	# owner: root
	# group: root
	user::rw-
	<e>user:nobody:rwx</e>
	group::r--
	mask::rwx
	other::r--

	# file: DST/plain_zeroed
	# owner: root
	# group: root
	user::rw-
	<e>user:nobody:rwx</e>
	group::r--
	mask::rwx
	other::r--

	<b>root@terre:/mnt/localdisk# getfattr -d SRC/plain_zeroed DST/plain_zeroed</b>
	# file: SRC/plain_zeroed
	<e>user.hello="hello world!!!"</e>

	# file: DST/plain_zeroed
	<e>user.hello="hello world!!!"</e>

	<b>root@terre:/mnt/localdisk/Benchmark_tools# lsattr SRC/plain_zeroed DST/plain_zeroed</b>
	<e>s---i-d-------e----</e> SRC/plain_zeroed
	<e>s---i-d-------e----</e> DST/plain_zeroed
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      To summarize:
    </p>
    <ul>
      <li>user and group ownership are restored</li>
      <li>permission are set correctly</li>
      <li>ACL are preperly restored</li>
      <li>Extended Attributes also</li>
      <li>file system specific attributes are too</li>
      <li>hard links are restored</li>
    </ul>
    <p>
      So what? Let's rerun <code>du</code> file by file:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du -B1 SRC/* DST/*</b>
	8192    SRC/SUB
	4096    SRC/dev
	0       SRC/fd1
	0       SRC/null
	<e>1048576 SRC/plain_zeroed</e>
	1052672 SRC/random
	8192    DST/SUB
	4096    DST/dev
	0       DST/fd1
	0       DST/null
	<e>4096    DST/plain_zeroed</e>
	1052672 DST/random
	<b>root@terre:/mnt/localdisk/Benchmark_tools# ls -l SRC/plain_zeroed DST/plain_zeroed</b>
	-rw-rwxr--+ 1 root root <e>1048576</e> Oct 21 18:40 DST/plain_zeroed
	-rw-rwxr--+ 1 root root <e>1048576</e> Oct 21 18:40 SRC/plain_zeroed
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      OK here is the explanation: <code>plain_zeroed</code> file was using 1048576 bytes of disk space in SRC
      but consumes only 4096 bytes in DST, but as its file size is still officially 1048576, it has become now a sparse file.
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# diff -s SRC/plain_zeroed DST/plain_zeroed</b>
	Files SRC/plain_zeroed and DST/plain_zeroed are <e>identical</e>
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>

    <p>
      But nothing changes from the user point of view, the restoration process with dar just optimized
      the space usage.
    </p>
    <p>
      Let's continue checking the inode dates. As you know, Unix inode have several dates:
      </p>
    <ul>
      <li><b>atime</b>, the access time, gives the last time the file's data has been accessed (read)</li>
      <li><b>mtime</b>, the modification time, gives the last time the file's data has been modified (wrote)</li>
      <li><b>ctime</b>, the change time, gives the last time the file's metadata in other word the inode properties (ownership, ACL, permissions, dates, ...) have been modified</li>
      <li><b>btime</b>, the birth time or yet creation time, gives the time the file has been created on the current filesystem, this date is not present on all Unix system</li>
    </ul>

    <p>
      The <code>ls -iRl</code> command we used so far does only show the <i>mtime</i> date moreover with
      a time accuracy of only one minute, while modern systems provide nanosecond precision. For that
      reason we will use the <code>stat</code> command instead to have all dates at the system time accuracy:
    </p>

    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# stat SRC/random DST/random</b>
	File: SRC/random
	Size: 1048576         Blocks: 2048       IO Block: 4096   regular file
	Device: 802h/2050d      Inode: 414840      Links: 1
	Access: (0644/-rw-r--r--)  Uid: (65534/  nobody)   Gid: (    0/    root)
	<e>Access: 2020-10-22 12:13:01.813319506 +0200</e>
	<e>Modify: 2020-10-22 12:12:57.765328555 +0200</e>
	<e class=red>Change: 2020-10-22 12:12:59.805323991 +0200</e>
	Birth: -
	File: DST/random
	Size: 1048576         Blocks: 2048       IO Block: 4096   regular file
	Device: 802h/2050d      Inode: 414889      Links: 1
	Access: (0644/-rw-r--r--)  Uid: (65534/  nobody)   Gid: (    0/    root)
	<e>Access: 2020-10-22 12:13:01.813319506 +0200</e>
	<e>Modify: 2020-10-22 12:12:57.765328555 +0200</e>
	<e class=red>Change: 2020-10-22 12:14:34.877131738 +0200</e>
	Birth: -
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>

    <p>
      From the above output we see that:
    </p>
    <ul>
      <li>atime is restored</li>
      <li>mtime is restored</li>
      <li>ctime is not restored</li>
    </ul>

    <p>
      As we targeted this benchmark mainly for Linux which has not yet the <code>btime</code> available
      (Well some Linux <a href="https://en.wikipedia.org/wiki/Comparison_of_file_systems#Metadata">file systems</a> support
      <i>btime</i> but its access is not yet fully available to applications), we will thus momentarily change to a BSD system
      to play with <code>btime</code>. BSD systems include MACOS X, FreeBSD, NetBSD, butterflyBSD,... we will use FreeBSD here.
      Under FreeBSD, the <code>stat</code> command is not as easy to read as under Linux, however it is
      very flexible which we will leverage to mimic the Linux output:
    </p>

    <div><code>
	<b>root@FreeBSD:~denis # which mystat</b>
	mystat:          aliased to <e>stat -f "%N%nAccess: %Sa%nModify: %Sm%nChange: %Sc%nBirth: %SB%n" !*</e>
	<b>root@FreeBSD:~denis # mystat SRC/random</b>
	SRC/random
	Access: Oct 27 13:28:41 2020
	Modify: Oct 22 15:34:07 2020
	Change: Oct 22 15:34:09 2020
	<e>Birth: Oct 22 15:34:07 2020</e>
	<b>root@FreeBSD:~denis # dar -c backup -R SRC -q</b>
	<b>root@FreeBSD:~denis # mkdir DST</b>
	<b>root@FreeBSD:~denis # dar -x backup -R DST -q</b>
	<b>root@FreeBSD:~denis # mystat DST/random</b>
	DST/random
	Access: Oct 27 13:28:41 2020
	Modify: Oct 22 15:34:07 2020
	Change: Oct 27 13:31:50 2020
	<e>Birth: Oct 22 15:34:07 2020</e>
	<b>root@FreeBSD:~denis #</b>
    </code></div>

    <p>
      In conclusion <i>dar</i> also saves and restores properly <code>btime</code>
    </p>


    <h4>Rsync</h4>
    <p>
      Let's do the same we did previously using <b>rsync</b>. We start by copying <code>SRC</code> directory to <code>DST</code>:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# chattr -i DST/plain_zeroed</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# rm -rf DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# mkdir DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# rsync -arvHAX SRC/* DST</b>
	sending incremental file list
	fd1
	null
	pipe
	plain_zeroed
	random
	SUB/
	SUB/hard_linked_pipe => pipe
	SUB/hard_linked_socket
	SUB/hard_linked_sparse_file
	SUB/symlink-broken -> random
	SUB/symlink-valid -> ../random
	dev/
	dev/log => SUB/hard_linked_socket
	sparse_file => SUB/hard_linked_sparse_file

	sent 12,340,909 bytes  received 173 bytes  24,682,164.00 bytes/sec
	total size is 22,577,173  speedup is 1.83
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      First note, the backup and restoration is done in one step, where <i>dar</i> was decorelating the backup operation
      from the restoration operation. The resulting backup needs not software to be restored (<code>DST</code>
      is a copy of <code>SRC</code>). For dar to reach the same result (without using storage for the backup) this
      implies two dar commands: <code> dar -c - -R SRC | dar -x - --sequential-read -R DSR</code>. The situation is
      similar with <code>tar</code>, you need two commands to performe the same task: <code>tar -cf - | tar -xf -</code>
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du -s SRC DST</b>
	2068    SRC
	<e>12064</e>   DST
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      Here, the space used in the restored backup is six time bigger than the original content! Let's continue
      as we did with dar:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# ls -iRl SRC DST</b>
	DST:
	total 12060
	414843 drwxr-xr-x  2 root   root     4096 Oct 28 11:09 SUB
	414844 drwxr-xr-x  2 root   root     4096 Oct 22 11:09 dev
	414840 brw-r--r--  1 root   root     2, 1 Oct 28 11:09 fd1
	414841 crw-r--r--  1 root   root     3, 1 Oct 28 11:09 null
	<e class=red>414842</e> prw-r--r--  2 root   root        0 Oct 28 11:09 pipe
	414848 -rw-rwxr--+ 1 root   root  1048576 Oct 28 11:09 plain_zeroed
	414849 -rw-r--r--  1 nobody root  1048582 Oct 28 11:09 random
	<e>414850</e> -rw-r--r--  2 root   root 10240000 Oct 28 11:09 sparse_file

	DST/SUB:
	total 10000
	<e class=red>414842</e> prw-r--r-- 2 root root          0 Oct 28 11:09 hard_linked_pipe
	<e class=blue>414845</e> srw-rw-rw- 2 root root          0 Oct 12 23:00 hard_linked_socket
	<e>414850</e> -rw-r--r-- 2 root root   10240000 Oct 28 11:09 hard_linked_sparse_file
	414846 lrwxrwxrwx 1 root root          6 Oct 28 11:09 symlink-broken -> random
	414847 lrwxrwxrwx 1 bin  daemon        9 Oct 28 11:09 symlink-valid -> ../random

	DST/dev:
	total 0
	<e class=blue>414845</e> srw-rw-rw- 2 root root 0 Oct 12 23:00 log

	SRC:
	total 2064
	411386 drwxr-xr-x  2 root   root     4096 Oct 28 11:09 SUB
	414836 drwxr-xr-x  2 root   root     4096 Oct 22 11:09 dev
	414835 brw-r--r--  1 root   root     2, 1 Oct 28 11:09 fd1
	414834 crw-r--r--  1 root   root     3, 1 Oct 28 11:09 null
	414832 prw-r--r--  2 root   root        0 Oct 28 11:09 pipe
	414826 -rw-rwxr--+ 1 root   root  1048576 Oct 28 11:09 plain_zeroed
	414827 -rw-r--r--  1 nobody root  1048582 Oct 28 11:09 random
	414828 -rw-r--r--  2 root   root 10240000 Oct 28 11:09 sparse_file

	SRC/SUB:
	total 4
	414832 prw-r--r-- 2 root root          0 Oct 28 11:09 hard_linked_pipe
	414837 srw-rw-rw- 2 root root          0 Oct 12 23:00 hard_linked_socket
	414828 -rw-r--r-- 2 root root   10240000 Oct 28 11:09 hard_linked_sparse_file
	414830 lrwxrwxrwx 1 root root          6 Oct 28 11:09 symlink-broken -> random
	414831 lrwxrwxrwx 1 bin  daemon        9 Oct 28 11:09 symlink-valid -> ../random

	SRC/dev:
	total 0
	414837 srw-rw-rw- 2 root root 0 Oct 12 23:00 log
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      All files are present in <code>DST</code> and use the expected space usage, as reported by the <code>ls</code>.
      We can also see that all three hard linked inode (plain file, socket and named pipe) are restored properly.
      So we can suspect the cause of the size difference to be linked with sparse files:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du SRC/sparse_file DST/sparse_file</b>
	4       SRC/sparse_file
	<e>10000</e>   DST/sparse_file
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      That's it, rsync restores sparse file as plain files.
    </p>
    <p>
      Let's now check file's metadata:
    </p>
    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# getfacl SRC/plain_zeroed DST/plain_zeroed</b>
	# file: SRC/plain_zeroed
	# owner: root
	# group: root
	user::rw-
	<e>user:nobody:rwx</e>
	group::r--
	mask::rwx
	other::r--

	# file: DST/plain_zeroed
	# owner: root
	# group: root
	user::rw-
	<e>user:nobody:rwx</e>
	group::r--
	group::rwx
	other::r--

	<b>root@terre:/mnt/localdisk/Benchmark_tools# getfattr -d SRC/plain_zeroed DST/plain_zeroed</b>
	# file: SRC/plain_zeroed
	<e>user.hello="hello world!!!"</e>

	# file: DST/plain_zeroed
	<e>user.hello="hello world!!!"</e>
	<br/>

	<b>root@terre:/mnt/localdisk/Benchmark_tools# lsattr SRC/plain_zeroed DST/plain_zeroed</b>
	<e class=red>s---i-d-------e----</e> SRC/plain_zeroed
	<e class=red>--------------e----</e> DST/plain_zeroed
	<b>root@terre:/mnt/localdisk/Benchmark_tools#stat SRC/random DST/random</b>
	File: SRC/random
	Size: 1048582         Blocks: 2056       IO Block: 4096   regular file
	Device: 802h/2050d      Inode: 414827      Links: 1
	Access: (0644/-rw-r--r--)  Uid: (65534/  nobody)   Gid: (    0/    root)
	<e class=red>Access: 2020-10-28 11:09:59.977926733 +0100</e>
	<e>Modify: 2020-10-28 11:09:57.973931318 +0100</e>
	<e class=red>Change: 2020-10-28 11:09:57.973931318 +0100</e>
	Birth: -
	File: DST/random
	Size: 1048582         Blocks: 2056       IO Block: 4096   regular file
	Device: 802h/2050d      Inode: 414849      Links: 1
	Access: (0644/-rw-r--r--)  Uid: (65534/  nobody)   Gid: (    0/    root)
	<e class=red>Access: 2020-10-28 12:07:53.622841733 +0100</e>
	<e>Modify: 2020-10-28 11:09:57.973931318 +0100</e>
	<e class=red>Change: 2020-10-28 12:07:53.622841733 +0100</e>
	Birth: -
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      So in summary:
    </p>
    <ul>
      <li>Permission are restored,</li>
      <li>user and group ownership are restored too,</li>
      <li>mtime is restored,</li>
      <li>File ACL are restored,</li>
      <li>Extended Attributes are restored</li>
    </ul>
    <p>
      But
    </p>
    <ul>
      <li>filesystem specific attributes are not restored,</li>
      <li>atime is not restored,</li>
      <li>ctime is not restored</li>
    </ul>

    <p>
      For <b>btime</b> as we did before, let's test under a FreeBSD system:
    </p>

    <div><code>
	<b>root@FreeBSD:~denis # rm -rf DST</b>
	<b>root@FreeBSD:/home/denis # which mystat</b>
	mystat:          aliased to stat -f "%N%nAccess: %Sa%nModify: %Sm%nChange: %Sc%nBirth: %SB%n" !*
	<b>root@FreeBSD:/home/denis # mystat SRC/random</b>
	SRC/random
	Access: Oct 27 14:27:59 2020
	Modify: Oct 22 15:34:07 2020
	Change: Oct 22 15:34:09 2020
	<e>Birth: Oct 22 15:34:07 2020</e>
	root@FreeBSD:/home/denis # mkdir DST
	root@FreeBSD:/home/denis # rsync -arv SRC/* DST
	sending incremental file list
	fd1
	null
	pipe
	plain_zeroed
	random
	sparse_file
	SUB/
	SUB/hard_linked_socket
	SUB/hard_linked_sparse_file
	SUB/symlink-broken -> random
	SUB/symlink-valid -> ../random
	dev/
	dev/log -> /var/run/log

	sent 22,583,283 bytes  received 129 bytes  45,166,824.00 bytes/sec
	total size is 22,577,179  speedup is 1.00
	root@FreeBSD:/home/denis # mystat DST/random
	DST/random
	Access: Oct 27 14:28:53 2020
	Modify: Oct 22 15:34:07 2020
	Change: Oct 27 14:28:53 2020
	<e>Birth: Oct 22 15:34:07 2020</e>
	<b>root@FreeBSD:/home/denis #</b>
    </code></div>
    <p>
      So, birthtime is properly restored.
    </p>

    <h4>Tar</h4>

    <p>
      As done with previously, let's save and restore the <code>SRC</code> directory to <code>DST</code>...
    </p>

    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# rm -rf DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# cd SRC</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools/SRC# tar -cf ../backup.tar *</b>
	tar: SUB/hard_linked_socket: socket ignored
	tar: dev/log: socket ignored
	<b>root@terre:/mnt/localdisk/Benchmark_tools/SRC# cd ../</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# mkdir DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# cd DST</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools/DST# tar -xf ../backup.tar</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools/DST# cd ..</b>
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>

    <p>
      ...and compare the restored data with the original:
    </p>

    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du -s SRC DST</b>
	2068    SRC
	12064   DST
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>

    <p>
      So we have here the same issue as with <i>rsync</i>, the restored data uses six time more space than the original data,
      most probable because of the sparse file not having been taken into account. But first we can check the warning
      <i>tar</i> issued at backup time:
    </p>

    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# ls -iRl SRC DST</b>
	DST:
	total 12060
	414841 drwxr-xr-x 2 root   root     4096 Oct 28 11:09 SUB
	414846 drwxr-xr-x 2 root   root     4096 Oct 22 11:09 dev
	414847 brw-r--r-- 1 root   root     2, 1 Oct 28 11:09 fd1
	414848 crw-r--r-- 1 root   root     3, 1 Oct 28 11:09 null
	414849 prw-r--r-- <e class=red>1</e> root   root        0 Oct 28 11:09 pipe
	414850 -rw-rwxr-- 1 root   root  1048576 Oct 28 11:09 plain_zeroed
	414852 -rw-r--r-- 1 nobody root  1048582 Oct 28 11:09 random
	<e>414843</e> -rw-r--r-- 2 root   root 10240000 Oct 28 11:09 sparse_file

	DST/SUB:
	total 10000
	414845 prw-r--r-- 1 root root          0 Oct 28 11:09 hard_linked_pipe
	<e>414843</e> -rw-r--r-- 2 root root   10240000 Oct 28 11:09 hard_linked_sparse_file
	414842 lrwxrwxrwx 1 root root          6 Oct 28 11:09 symlink-broken -> random
	414844 lrwxrwxrwx 1 bin  daemon        9 Oct 28 11:09 symlink-valid -> ../random

	DST/dev:
	total 0

	SRC:
	total 2064
	411386 drwxr-xr-x  2 root   root     4096 Oct 28 11:09 SUB
	414836 drwxr-xr-x  2 root   root     4096 Oct 22 11:09 dev
	414835 brw-r--r--  1 root   root     2, 1 Oct 28 11:09 fd1
	414834 crw-r--r--  1 root   root     3, 1 Oct 28 11:09 null
	414832 prw-r--r--  <e class=red>2</e> root   root        0 Oct 28 11:09 pipe
	414826 -rw-rwxr--+ 1 root   root  1048576 Oct 28 11:09 plain_zeroed
	414827 -rw-r--r--  1 nobody root  1048582 Oct 28 11:09 random
	414828 -rw-r--r--  2 root   root 10240000 Oct 28 11:09 sparse_file

	SRC/SUB:
	total 4
	414832 prw-r--r-- 2 root root          0 Oct 28 11:09 hard_linked_pipe
	<e class=red>414837 srw-rw-rw- 2 root root          0 Oct 12 23:00 hard_linked_socket</e>
	414828 -rw-r--r-- 2 root root   10240000 Oct 28 11:09 hard_linked_sparse_file
	414830 lrwxrwxrwx 1 root root          6 Oct 28 11:09 symlink-broken -> random
	414831 lrwxrwxrwx 1 bin  daemon        9 Oct 28 11:09 symlink-valid -> ../random

	SRC/dev:
	total 0
	<e class=red>414837 srw-rw-rw- 2 root root 0 Oct 12 23:00 log</e>
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>

    <p>
      The warning was not vain, <code>SUB/hard_linked_socket</code> and <code>log</code> are missing in <code>DST</code>.
      This is however a minor problem as usually unix sockets get recreated by the process using them. However
      we might have some permission and ownership to set back, by hand. A possile use case is <code>syslog</code> daemon,
      when let available for a chrooted process or container (MTA, or other network service).
    </p>
    <p>
      The second problem is a bit more annoying: the hard linked fifo (aka named pipe)
      is silently restored as two independent named pipes (the inode number are different in the first column
      for <code>pipe</code> and <code>SUB/hard_linked_pipe</code> and their respective link count was <code>2</code>
      in <code>SRC</code> but is now <code>1</code> in <code>DST</code>. If two processes in different namespaces or
      chrooted environment, exchange data by mean of such hardlinked pipe, after restoration, if you are not
      aware of this restriction, it will be difficult to identify why the two process are just locked out, one
      waiting for data that will never come from the pipe, the other stuck for the pipe to be read.
    </p>

    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# du SRC/sparse_file DST/sparse_file</b>
	4       SRC/sparse_file
	<e>10000</e>   DST/sparse_file
	<b>root@terre:/mnt/localdisk/Benchmark_tools#</b>
    </code></div>
    <p>
      The reason of the size increase of the restored data finds its root in the absence of
      sparse file consideration. Let's continue by checking the file's metadata:
    </p>

    <div><code>
	<b>root@terre:/mnt/localdisk/Benchmark_tools# getfacl SRC/plain_zeroed DST/plain_zeroed</b>
	# file: SRC/plain_zeroed
	# owner: root
	# group: root
	user::rw-
	<e class=red>user:nobody:rwx</e>
	group::r--
	mask::rwx
	other::r--

	# file: DST/plain_zeroed
	# owner: root
	# group: root
	user::rw-
	group::rwx
	other::r--

	<b>root@terre:/mnt/localdisk/Benchmark_tools# getfattr -d SRC/plain_zeroed DST/plain_zeroed</b>
	# file: SRC/plain_zeroed
	<e class=red>user.hello="hello world!!!"</e>

	<b>root@terre:/mnt/localdisk/Benchmark_tools# lsattr SRC/plain_zeroed DST/plain_zeroed</b>
	<e class=red>s---i-d-------e----</e> SRC/plain_zeroed
	<e class=red>--------------e----</e> DST/plain_zeroed
	<b>root@terre:/mnt/localdisk/Benchmark_tools# stat SRC/random DST/random</b>
	File: SRC/random
	Size: 1048576         Blocks: 2048       IO Block: 4096   regular file
	Device: 802h/2050d      Inode: 414841      Links: 1
	Access: (0644/-rw-r--r--)  Uid: (65534/  nobody)   Gid: (    0/    root)
	<e class=red>Access: 2020-10-27 14:03:46.064046436 +0100</e>
	<e>Modify: 2020-10-27 14:03:42.016050420 +0100</e>
	<e class=red>Change: 2020-10-27 14:03:44.048048418 +0100</e>
	Birth: -
	File: DST/random
	Size: 1048576         Blocks: 2048       IO Block: 4096   regular file
	Device: 802h/2050d      Inode: 414890      Links: 1
	Access: (0644/-rw-r--r--)  Uid: (65534/  nobody)   Gid: (    0/    root)
	<e class=red>Access: 2020-10-27 19:08:14.932424226 +0100</e>
	<e>Modify: 2020-10-27 14:03:42</e>.<e class=red>000000000 +0100</e>
	<e class=red>Change: 2020-10-27 19:08:14.932424226 +0100</e>
	Birth: -
	root@terre:/mnt/localdisk/Benchmark_tools#
    </code></div>

    <p>
      From the above output we see that:
    </p>
    <ul>
      <li>permission are restored,</li>
      <li>user and group ownership are restored too,</li>
      <li>mtime is restored but rounded at the nearest second while today's system time accuracy is the nanosecond</li>
    </ul>

    <p>
      But
    </p>

    <ul>
      <li>ACL are not restored,</li>
      <li>Extended Attributes are not restored,</li>
      <li>filesystem attributes are not restored,<li>
      <li>atime is not restored,</li>
      <li>ctime is not restored</li>
    </ul>

    <p>
      For the last date, <b>birthtime</b> again we will perform the test under FreeBSD:
    </p>

    <div><code>
	<b>root@FreeBSD:~denis # which mystat</b>
	mystat:          aliased to stat -f "%N%nAccess: %Sa%nModify: %Sm%nChange: %Sc%nBirth: %SB%n" !*
	<b>root@FreeBSD:~denis # mystat SRC/random</b>
	SRC/random
	Access: Oct 27 19:40:13 2020
	Modify: Oct 22 15:34:07 2020
	Change: Oct 22 15:34:09 2020
	<e>Birth: Oct 22 15:34:07 2020</e>
	<b>root@FreeBSD:~denis # cd SRC</b>
	<b>root@FreeBSD:~denis/SRC # gtar -cf ../backup.tar random</b>
	<b>root@FreeBSD:~denis/SRC # cd ..</b>
	<b>root@FreeBSD:~denis # mkdir DST</b>
	<b>root@FreeBSD:~denis # cd DST</b>
	<b>root@FreeBSD:~denis/DST # tar -xf ../backup.tar</b>
	<b>root@FreeBSD:~denis/DST # cd ..</b>
	<b>root@FreeBSD:~denis # mystat DST/random</b>
	DST/random
	Access: Oct 28 15:43:30 2020
	Modify: Oct 22 15:34:07 2020
	Change: Oct 28 15:43:30 2020
	<e>Birth: Oct 22 15:34:07 2020</e>
	<b>root@FreeBSD:~denis #</b>
    </code></div>

    <p>
      <i>gtar</i> saved and restored the birthtime
    </p>



    <h3>Execution Performance</h3>

    <p>
      In the following to compare software while they have different level of feature (<i>dar</i>
      is the only one to take into account sparse files, <i>dar</i> and <i>rsync</i> are the only
      one to backup rich metadata like Exended Attributes, ACL, ...) we will provide measures for
      <i>dar</i> in <u>normal mode</u> and in <u>fast mode</u>. In this later mode, the inlined metadata
      redundancy and the sparse file detection are disabled to have a similar features set as
      <i>rsync</i> and closer to <i>tar</i> feature level.
    </p>

    <h4>Performance test on an debian iso file</h4>
    <p>
      This first test's goal is to measure the performance of treatment for a huge file
    </p>
    <p>
      After having created a tmpfs filesystem mounted at /mnt/memdisk, we have
      disabled swapping with <code>swapoff -a</code> and setup an <code>SRC</code>
      directory with the ISO image in it:
      </p>
    <div><code>
	devuan | /mnt/memdisk >mkdir SRC
	devuan | /mnt/memdisk >cp ~denis/tmp/debian-10.6.0-amd64-DVD-2.iso SRC
	devuan | /mnt/memdisk >ls -l SRC
	total 4577072
	-rw-r--r-- 1 denis denis <e>4686921728</e> Oct 29 17:56 debian-10.6.0-amd64-DVD-2.iso
	devuan | /mnt/memdisk >
    </code></div>

    <h5>Full backup and restoration without compression</h5>

    <p>
      The main use case of backup without compression is to copy data locally,
      sort of equivalent of the linux <i>cp</i> which execution time is given below for reference.
      For backup that you want to keep some time, you will most
      probably use compression to save disk space because the compression time is
      negligeable compared to the backup retention time.
      For <i>rsync</i> (which finally does a file copy) compression
      is only interesting when performing the backup+restoration
      operation over a low bandwidth network (which is the first target of this
      software as its name <i>remote sync</i> states). In that context,
      the compression time is less important than the data transfert delay reduction,
      the slowest operation stays then the data transfer over the network.
    </p>
    <div><code>
	devuan:/mnt/memdisk# time cp --preserve -R SRC DST
	0.040u 2.840s <e>0:02.88</e> 100.0%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      The <i>cp</i> operation which can be seen as the fastest possible operation
      of the backup+restoration cycle is of 2.88 seconds only.
    </p>

    <div><code>
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q
	13.167u 2.807s <e>0:15.98</e> 99.8%    0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >time dar -c backup-at -R SRC -q -at
	8.873u 2.644s <e>0:11.52</e> 99.9%     0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >time dar -c backup-at-1-0 -R SRC -q -at -1 0
	1.658u 2.910s <e>0:04.58</e> 99.5%     0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >ls -l backup*.dar
	-rw-r--r-- 1 root root <e>4686922021</e> Oct 29 16:05 backup-at-1-0.1.dar
	-rw-r--r-- 1 root root <e>4678263075</e> Oct 29 16:04 backup-at.1.dar
	-rw-r--r-- 1 denis denis <e>4678263229</e> Oct 29 16:03 backup.1.dar
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup -R DST -q
	10.961u 2.857s <e>0:13.83</e> 99.8%    0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >rm -rf DST
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup-at-1-0 -R DST -q
	1.617u 2.793s <e>0:04.68</e> 94.0%     0+0k 7760+0io 45pf+0w
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      For <i>dar</i> the operation took:
    </p>
    <ul>
      <li>15.98 seconds for backup</li>
      <li>11.52 seconds for backup disabling the metadata redundancy</li>
      <li>4.66  seconds for backup disabling sparse file detection in addition</li>
      <li>13.83 seconds for restoration</li>
      <li>4.68  seconds to restore when no metadata redundancy and sparse file was actived</li>
      <li>thus 29.83 seconds for the backup + restoration with <u>default mode</u></li>
      <li>or only 9.34 seconds <u>in fast</u> mode</li>
      <li>the backup size is 99,81% of the size of the saved data</li>
    </ul>
    <p>
      We see the performance impact of the sparse file detection feature and
      of the inlined metadata redundancy, feature not present in the other
      software we compare it to here.
    </p>
    <br/>

    <div><code>
	devuan | /mnt/memdisk >time tar -cf backup.tar SRC
	0.419u 2.909s <e>0:03.36</e> 98.5%     0+0k 1000+0io 3pf+0w
	devuan | /mnt/memdisk >rm -rf DST
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >cd DST
	devuan | /mnt/memdisk/DST >time tar -xf ../backup.tar
	0.335u 2.987s <e>0:03.32</e> 99.6%     0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >cd ..
	devuan | /mnt/memdisk >ls -l backup.tar
	-rw-r--r-- 1 denis denis <e>4686929920</e> oct.  29 16:00 backup.tar
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      For <i>tar</i> the operation took:
    </p>
    <ul>
      <li>near 3.36 seconds for backup</li>
      <li>and  3.32 seconds for restoration</li>
      <li>thus 6.68 seconds for the backup + restoration</li>
      <li>the backup size is a bit more than 100% of the size of the saved data</li>
    </ul>
    <br/>
    <div><code>
	devuan | /mnt/memdisk >rm -rf DST
	devuan | /mnt/memdisk >time rsync -arvHAXq SRC/* DST
	22.837u 5.597s <e>0:15.30</e> 185.7%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>

    <p>
      For <i>rsync</i> the operation took:
    </p>
    <ul>
      <li>near 15.30 seconds for backup and restoration equivalent operation</li>
      <li>
	the backup size is a exactly 100% of the size of the saved data
	(1 to 1 copied files)
      </li>
    </ul>
    <p>
      Interesting point, <i>rsync</i> used several threads (185.7% of execution time)
      while just below 100% (1 thread).
    </p>
    <p>
      <u>In conclusion</u> on this part, when not using compression, thus targetting a file copy,
      depending on the file's nature (sparse file) and inode metadata (EA,...) to copy
      (see previous test category for details), you would probably prefer using <i>cp</i>.
      But to take into account metadata <i>dar</i> in fast mode, is faster than <i>rsync</i>
      when the same level of feature is used. If sparse file has to be used, <i>dar</i> is
      the only solution that match in that case.
    </p>

    <h5>Full backup and restoration with compression</h5>

    <p>
      If using compression the space resulting space gain is the objective, beside the
      fact to store for data for a relatively long term compared to the backup operation itself.
      The other use of compression is the remote copy over a reduced bandwidth network. We use
      the <i>gzip</i> compression algorithm because it is widely available with its default
      compression level of 6.
    </p>

    <div><code>
	devuan | /mnt/memdisk >time dar -c backup -z:6 -R SRC -q
	140.025u 2.621s <e>2:22.90</e> 99.8%   0+0k 3504+0io 25pf+0w
	devuan | /mnt/memdisk >time dar -c backup-at-1-0 -z:6 -R SRC -q -at -1 0
	130.247u 2.879s <e>2:13.14</e> 99.9%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >ls -l
	total 9134704
	drwxr-xr-x 2 denis denis         60 Nov  1 15:42 SRC
	-rw-r--r-- 1 denis denis <e>4677038814</e> Nov  1 16:13 backup-at-1-0.1.dar
	-rw-r--r-- 1 denis denis <e>4676967255</e> Nov  1 16:07 backup.1.dar
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup -R DST -q
	13.724u 2.711s <e>0:16.45</e> 99.8%    0+0k 256+0io 1pf+0w
	devuan | /mnt/memdisk >rm -rf DST
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup-at-1-0 -R DST -q
	4.535u 2.690s <e>0:07.23</e> 99.8%     0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      For <i>dar</i> the operation took:
    </p>
    <ul>
      <li>142.90 seconds for backup with the default mode</li>
      <li>133.14 seconds for backup with the <u>fast mode</u></li>
      <li>16.45 seconds for backup with the default_mode</li>
      <li>7.23 seconds for backup with the <u>fast mode</u></li>
      <li>which makes a total of 159.15 seconds for backup+restoration</li>
      <li>or 140,37 seconds in <u>fast mode</u></li>
      <li>the backup size is 99.78% of the saved data (compression ratio is 0,22%)
    </ul>
    <br/>

    <div><code>
	devuan | /mnt/memdisk >time tar -czf backup.tar.gz SRC
	132.220u 10.926s <e>2:13.27</e> 107.4% 0+0k 192+0io 1pf+0w
	devuan | /mnt/memdisk >ls -l backup.tar.gz
	-rw-r--r-- 1 denis denis <e>4676242880</e> Oct 29 16:18 backup.tar.gz
	devuan | /mnt/memdisk >rm -rf SRC
	devuan | /mnt/memdisk >time tar -xzf backup.tar.gz SRC
	21.993u 7.478s <e>0:23.14</e> 127.3%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>

    <p>
      For <i>tar</i> the operation took:
    </p>
    <ul>
      <li>133.27 seconds for backup</li>
      <li>23.14 seconds for restoration</li>
      <li>which makes a total time of 156.41 seconds</li>
      <li>the backup size is 99,77% of the saved data (compression ratio is 0,23%)
    </ul>
    <br/>

    <div><code>
	devuan | /mnt/memdisk >rmdir DST
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time rsync -arvHAXqz SRC/* DST
	157.086u 8.448s 2:11.10 126.2%  0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      It may sound strange to use compression with rsync for local copy: we just compare here
      the performance variation on how these softwares leverage compression. Network latency will
      add some delay that may lead the compression time to become negligeable, or, if network is
      faster, to be the weak link.
    </p>
    <p>
      For <i>rsync</i> the operation took:
    </p>
    <ul>
      <li>131.04 seconds for combined backup-restoration</li>
      <li>compression used does not result in any gain in disk space</li>
    </ul>
    <p>
      <u>In conclusion</u> for this part, we have similar execution time, and similar compression
      ratio for all. <i>rsync</i> is slightly faster than <i>dar</i> when this one is used in fast mode,
      and <i>tar</i> is slightly faster than dar when this one is used in normal mode (but without
      taking care of rich metadata and sparse files). For remote copy over a slow network,
      <i>rsync</i> seems the best choice, while for storage of compressed backup
      <i>dar</i> is the best choice in all cases.
    </p>

    <h5>Full backup and restoration with compression using multiple threads</h5>

    <p>
      This test is just for fun, as no comparison is possible because only <i>dar</i>
      between these tools is able to leverage
      multiple cores to boost compression time operation (not the compression
      ratio which depends on the data to compress).
    </p>
    <p>
      Setting compression block of 1 MiB with 8 worker threads we get the following:
    </p>
    <div><code>
	devuan | /mnt/memdisk >rm -rf DST
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q -z:6:1M -G 8
	164.334u 3.342s <e>0:18.10</e> 926.3%  0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup -R DST -q -G 8
	23.048u 3.876s <e>0:09.36</e> 287.5%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >ls -l backup.1.dar
	-rw-r--r-- 1 denis denis <e>4677031125</e> Nov  1 16:40 backup.1.dar
	devuan | /mnt/memdisk >
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q -z:6:1M -G 8 -at -1 0
	145.621u 4.616s <e>0:17.75</e> 846.3%  0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup -R DST -q -G 8
	11.981u 4.249s <e>0:04.42</e> 366.9%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      In normal mode the backup+restoration has reduced from 159.15 seconds as seen before
      to 27,46 seconds and down to 22,17 seconds in <u>fast mode</u>.
    </p>
    <p>
      The HPE server I had the chance to access to do these performance tests on
      (a <i>ProLiant XL230a Gen9</i>
      running two <i>Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz</i> processors)
      has a lot of cores, so I could test multi-threading with 16 and even 24 worker and
      threads:
    </p>

    <div><code>
	devuan | /mnt/memdisk >rm -rf DST backup.*
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q -z:6:1M -G 16
	182.396u 3.089s <e>0:11.59</e> 1600.2% 0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup -R DST -G 16 -q
	27.472u 3.978s <e>0:09.44</e> 333.0%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >rm -rf DST backup.*
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q -z:6:1M -G 16 -at -1 0
	154.029u 3.723s <e>0:09.50</e> 1660.4% 0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -x backup -R DST -G 16 -q
	16.476u 4.478s <e>0:04.44</e> 471.6%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      The time gain rawly follows by the number of thread, the process overhead is thus very
      little. However when sparse file detection is active the backup execution time took
      11,59 seconds instead of the 9 expected ones: Thanks to parallelization the compression
      cost start to become negligible compared to the sparse file detection operation (which is
      not parallelized).
    </p>
    <p>
      For reference the same operation with 24 threads:
    </p>
    <div><code>
	devuan | /mnt/memdisk >rm -rf backup.1.dar DST
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q -z:6:1M -G 24
	225.665u 3.434s <e>0:11.76</e> 1948.0% 0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >time dar -x backup -R DST -G 24 -q
	35.082u 4.688s <e>0:10.29</e> 386.3%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >rm -rf backup.1.dar DST
	devuan | /mnt/memdisk >mkdir DST
	devuan | /mnt/memdisk >time dar -c backup -R SRC -q -z:6:1M -G 24 -at -1 0
	169.115u 4.160s <e>0:07.05</e> 2457.7% 0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >time dar -x backup -R DST -G 24 -q
	19.244u 4.793s <e>0:04.43</e> 542.4%   0+0k 0+0io 0pf+0w
	devuan | /mnt/memdisk >
    </code></div>
    <p>
      <u>In conclusion</u> for this test, if compression ratio <b>and</b> and execution time is
      the target,
      <i>dar</i> provides the best solution: it is the only one in between the software tested
      that seem able to leverage multicore processor, which are very common today. This is true
      in particular
      when there is a good portion of big files, because small files compression cannot leveral
      multi-thread (compression block size used here is 1 MiB, reducing it will degrade overall
      compression ratio).
    </p>

    <h4>Performance test on linux kernel pristine source code tree</h4>

    <p>
      For this test, we perform a backup and restoration of a lot of small files.
      Then we will also measure the time to restore only a few files.
    </p>

    <div><code>
	devuan:/mnt/memdisk# mkdir SRC
	devuan:/mnt/memdisk# cd SRC
	devuan:/mnt/memdisk/SRC# tar -xf ~denis/tmp/linux-5.9.2.tar.xz
	devuan:/mnt/memdisk/SRC# cd ..
	devuan:/mnt/memdisk# du -B1 -s SRC
	<e>1121144832</e>      SRC
	devuan:/mnt/memdisk#

    </code></div>

    <h5>Full backup and restoration without compression</h5>

    <p>
      As stated in the previous test, the use case of backup without compression
      is short term backup or file copy. Below is, for reference, the execution
      of the <i>cp</i> command:
    </p>
    <div><code>
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time cp --preserve -R SRC DST
	0.364u <e>1.487s</e> 0:01.87 98.3%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      <i>cp</i> performs the copy of the Linux source tree in 1.48 seconds
    </p>

    <div><code>
      devuan:/mnt/memdisk# time dar -c backup -R SRC -q
      4.687u 1.335s <e>0:06.04</e> 99.5%     0+0k 0+0io 0pf+0w
      devuan:/mnt/memdisk# ls -l backup.1.dar
      -rw-r--r-- 1 root root <e>967464861</e> Oct 30 18:23 backup.1.dar
      devuan:/mnt/memdisk# mkdir DST
      devuan:/mnt/memdisk# time dar -x backup -R DST -q
      3.525u 1.834s <e>0:05.37</e> 99.6%     0+0k 0+0io 0pf+0w
      devuan:/mnt/memdisk# time dar -x backup -R DST -q -w -g SRC/linux-5.9.2/CREDITS
      1.156u 0.553s <e>0:01.71</e> 99.4%     0+0k 0+0io 0pf+0w
      devuan:/mnt/memdisk#
    </code></div>
    <p>
      and in fast mode:
    </p>
    <div><code>
	devuan:/mnt/memdisk# rm -rf backup* DST ; mkdir DST
	devuan:/mnt/memdisk# time dar -c backup -R SRC -q -at -1 0
	1.951u 1.391s <e>0:03.36</e> 99.4%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ls -l backup.1.dar
	-rw-r--r-- 1 root root <e>960787709</e> Oct 30 18:27 backup.1.dar
	devuan:/mnt/memdisk# time dar -x backup -R DST -q
	1.659u 1.294s <e>0:02.97</e> 98.9%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -x backup -R DST -q -w -g SRC/linux-5.9.2/CREDITS
	0.409u 0.044s <e>0:00.46</e> 95.6%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      For <i>dar</i> the operation took:
    </p>
    <ul>
      <li>6.04 seconds for backup</li>
      <li>3.36 seconds for backup in <u>fast mode</u></li>
      <li>5.37 seconds for full restoration</li>
      <li>2.97 seconfs for full restoration in <u>fast mode</u></li>
      <li>11.41 s for the full backup and restoration</li>
      <li>6.33 seconds for full backup+restoration in <u>fast mode</u></li>
      <li>1.71 second to restore a single file</li>
      <li>0.44 second to restore a single file in <u>fast mode</u></li>
      <li>the resulting backup space is 86,3 % of the original data</li>
      <li>and 85,7 % of the original data in <u>fast mode</u></li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# time tar -cf backup.tar SRC
	0.337u 0.967s <e>0:01.31</e> 98.4%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ls -l backup.tar
	-rw-r--r-- 1 root root <e>1011200000</e> Oct 30 18:25 backup.tar
	devuan:/mnt/memdisk# rm -rf DST
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# cd DST
	devuan:/mnt/memdisk/DST# time tar -xf ../backup.tar
	0.362u 1.182s <e>0:01.55</e> 99.3%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST# time tar -xf ../backup.tar SRC/linux-5.9.2/CREDITS
	0.178u 0.095s <e>0:00.27</e> 96.2%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST#
    </code></div>
    <p>
      For <i>tar</i> the operation took:
    </p>
    <ul>
      <li>1.31 second for backup</li>
      <li>1.55 second for the restoration</li>
      <li>2.86 seconds for the full backup and restoration</li>
      <li>0.27 second for the restoration of a single file</li>
      <li>the resulting backup uses 90.2 % of the original data</li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# rm -rf DST
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time rsync -arvHAXq SRC/* DST
	6.480u 3.683s <e>0:05.64</e> 180.1%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# rm DST/linux-5.9.2/CREDITS
	devuan:/mnt/memdisk# time rsync -arvHAXq SRC/linux-5.9.2/CREDITS DST
	0.001u 0.004s <e>0:00.00</e> 0.0%      0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      For <i>rsync</i> the operation took:
    </p>
    <ul>
      <li>5.64 seconds for the equivalent of a full backup and restoration</li>
      <li>0.005 seconds for the restoration of a single file(!)</li>
      <li>the resulting backup uses 100 % of the original data</li>
    </ul>

    <p>
      <u>In conclusion</u>, Without compression, <i>dar</i> is the slowest but gives the better
      space optimization of the resulting backup. At the opposite, <i>tar</i> is the fastest for
      full restoration and <i>rsync</i> is extremely fast when it comes to backup and restore a
      single file.
      <br/>
      In this scenario without compression, the use case is either:
      <ul>
	<li>
	  local copy for a full backup: <i>cp</i> gives the best performance but as <i>tar</i>
	  does not hold rich metadata, in that case <i>rsync</i> has to be used. If sparse file
	  have to be taken into account on top, <i>dar</i> in normal mode is necessary.
	</li>
	<li>
	  or remote copy over a large bandwidth network: the best is <i>tar</i> with usual network
	  protocol tool (sftp, ftp, ...). If you do not bother ciphering your data over the network,
	  <i>rsync</i> is a bit slower but probably simpler to use in that case.
	</li>
      </ul>
    </p>

    <h5>Full backup and restoration with compression</h5>

    <p>
      As stated for on the first data set, using compression makes sense for space disk
      preservation and somehow long term retention and/or backup over a low bandwidth network. In
      this scenario, the execution time is less important.
    </p>

    <div><code>
	devuan:/mnt/memdisk# time dar -c backup -R SRC -q -z6
	23.890u 1.125s <e>0:25.03</e> 99.9%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -c backup-at-1-0 -R SRC -q -z6 -at -1 0
	21.800u 1.039s <e>0:22.85</e> 99.9%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ls -l backup*
	-rw-r--r-- 1 root root <e>212368008</e> Nov  1 17:41 backup-at-1-0.1.dar
	-rw-r--r-- 1 root root <e>219047659</e> Nov  1 17:39 backup.1.dar
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time dar -x backup -R DST -q
	4.995u 1.179s <e>0:06.19</e> 99.5%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -x backup -R DST -q -w -g SRC/linux-5.9.2/CREDITS
	0.632u 0.020s <e>0:00.66</e> 98.4%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      For <i>dar</i> the operation took:
    </p>
    <ul>
      <li>25.03 seconds for backup</li>
      <li>22.85 seconds for backup in <u>fast mode</u></li>
      <li>6.32 seconds for restoration (same time in fast mode)</li>
      <li>which makes 31.35 seconds for the full backup+restoration cycle</li>
      <li> and only 29,17 seconds in <u>fast mode</u></li>
      <li>0.65 seconds for the restoration of a single file</li>
      <li>the backup provides a compression ratio of 80.4 %</li>
      <li> and 81 % in <u>fast mode</u></li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# time tar -czf backup.tar.gz SRC
	25.394u 2.135s <e>0:24.34</e> 113.0%   0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ls -l backup.tar.gz
	-rw-r--r-- 1 root root <e>183630552</e> Oct 30 18:48 backup.tar.gz
	devuan:/mnt/memdisk# rm -rf DST
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# cd DST
	devuan:/mnt/memdisk/DST# time tar -xf ../backup.tar.gz
	5.228u 1.771s <e>0:04.95</e> 141.2%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST# time tar -xf ../backup.tar.gz SRC/linux-5.9.2/CREDITS
	5.069u 0.554s <e>0:04.90</e> 114.4%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST#
    </code></div>
    <p>
      For <i>tar</i> the operation took:
    </p>
    <ul>
      <li>24.34 seconds for backup</li>
      <li>4.95 seconds for restoration</li>
      <li>which makes 29.29 seconds for the full backup+restoration cycle</li>
      <li>4.95 seconds for the restoration of a single file</li>
      <li>the backup provides a compression ratio of 83.6 %</li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# rm -rf DST
	devuan:/mnt/memdisk# time rsync -arvHAXqz SRC/* DST
	30.719u 3.195s <e>0:22.59</e> 150.0%   0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# rm DST/linux-5.9.2/CREDITS
	devuan:/mnt/memdisk# time rsync -arvHAXqz SRC/linux-5.9.2/CREDITS  DST
	0.014u 0.003s <e>0:00.01</e> 100.0%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      For <i>rsync</i> the operation took:
    </p>
    <ul>
      <li>22.59 seconds for the full backup+restoration cycle</li>
      <li>0.01 for the restoration of a single file</li>
      <li>
	the backup provides a compression ratio of 0 % (no compression as the
	compression is used in flight for the data transfer, but what remains
	is a plain copy of the backed up data)
      </li>
    </ul>
    <p>
      <u>In conclusion</u> for this test, we have two use cases:
      <ul>
	<li>
	  For relatively long term storage (compared to the
	  execution time of the operation), <i>tar</i> and <i>dar</i> perform equivalently in
	  term of compression ratio, however <i>dar</i> very quick or
	  restoration of a few files. <i>rsync</i> at the opposite is not adapted here,
	  as compression does not last the resulting data is not compressed.
	</li>
	<li>
	  For transfer the compression and decompression with <i>tar</i> or <i>dar</i> take place
	  on two different computer, the overall time for the operation is thus the slowest operation
	  (the backup) which duration is equivalent to the one of <i>rsync</i>. However <i>rsync</i>
	  is simpler to use if no data ciphering is required over the network and the data can
	  be stored in clear on the remote host. Last if rich metadata and/or spares file are involved
	  <i>dar</i> is the best choice.
	</li>
      </ul>
    </p>


    <h4>Differential backup and restoration</h4>

    <p>
      Three even can lead to requiring a new backup:
    </p>
    <ul>
      <li>new file has been added</li>
      <li>an existing file has been modified</li>
      <li>a file has been removed</li>
    </ul>
    <p>
      we will use the first data set (Debian ISO image) for the second case,
      add a small file and remove an already backup file using the second
      data set (kernel image).
    </p>
    <p>
      We will also use compression as it makes sense for relatively long term
      backup (short term backup are destroyed thus differential backup does not
      makes sense in that context, we should thus discard <i>rsync</i> from this
      test as it does not store saved data compressed. We will keep it in the
      test for reference).
    </p>

    <h5>Modified file</h5>

    <div><code>
	devuan:/mnt/memdisk# time dar -c full -z6 -R SRC --delta sig -q
	260.927u 142.367s <e>6:43.39</e> 99.9% 0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -c full-at-1-0 -z6 -R SRC --delta sig -q -at -1 0
	266.892u 140.384s <e>6:47.35</e> 99.9% 0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -c full-fast -z6 -R SRC -q -at -1 0
	129.998u 3.019s <e>2:13.03</e> 99.9%   0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ./bitflip 100000 SRC/debian-10.6.0-amd64-DVD-2.iso
	devuan:/mnt/memdisk# time dar -c diff -A full -z6 -R SRC -q
	11.171u 2.387s <e>0:13.57</e> 99.8%    0+0k 24+0io 1pf+0w
	devuan:/mnt/memdisk# ls -l *.dar
	-rw-r--r-- 1 root root        <e>643</e> Nov  1 19:45 diff.1.dar
	-rw-r--r-- 1 root root 4704501307 Nov  1 19:23 full-at-1-0.1.dar
	-rw-r--r-- 1 root root 4677038836 Nov  1 19:30 full-fast.1.dar
	-rw-r--r-- 1 root root <e>4704429776</e> Nov  1 19:05 full.1.dar
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time dar -x full -R DST -q
	13.913u 2.689s <e>0:16.61</e> 99.8%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -x diff -R DST -q
	3.101u 6.715s <e>0:09.83</e> 99.7%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# diff -s SRC/debian-10.6.0-amd64-DVD-2.iso DST/debian-10.6.0-amd64-DVD-2.iso
	Files SRC/debian-10.6.0-amd64-DVD-2.iso and DST/debian-10.6.0-amd64-DVD-2.iso are identical
	devuan:/mnt/memdisk#
    </code></div>

    <p>
      For <i>dar</i> the operation took:
    </p>
    <ul>
      <li>403.39 seconds for the full backup with delta signature</li>
      <li>133.03 seconds for normal full backup</li>
      <li>13.57 seconds for the differential backup with binary delta</li>
      <li>4.3 GiB for the full backup (compression ratio of 0,21 %)</li>
      <li>614 bytes for the differential backup (compression ratio of 99,99998%)</li>
      <li>16.61 seconds to restore the full backup</li>
      <li>9.83 seconds to restore the differential backup</li>
      <li>420.00 seconds the full backup+restoration</li>
      <li>23,40 seconds for differential backup+restoration</li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# time tar --listed-incremental=snapshot.file -czf full.tar.gz SRC
	129.905u 10.891s <e>2:11.08</e> 107.4% 0+0k 208+0io 2pf+0w
	devuan:/mnt/memdisk#  ./bitflip 100000 SRC/debian-10.6.0-amd64-DVD-2.iso
	devuan:/mnt/memdisk# time tar --listed-incremental=snapshot.file -czf diff.tar.gz SRC
	129.932u 10.677s <e>2:11.16</e> 107.1% 0+0k 288+0io 4pf+0w
	devuan:/mnt/memdisk# ls -l
	total 9133304
	drwxr-xr-x 2 root root         40 Oct 31 17:31 SRC
	-rwxr--r-- 1 root root        460 Oct 31 16:34 bitflip
	-rw-r--r-- 1 root root <e>4676243904</e> Oct 31 17:28 diff.tar.gz
	-rw-r--r-- 1 root root <e>4676244172</e> Oct 31 17:24 full.tar.gz
	-rw-r--r-- 1 root root        107 Oct 31 17:28 snapshot.file
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# cd DST
	devuan:/mnt/memdisk/DST# time tar --listed-incremental=/dev/null -xf ../full.tar.gz
	22.358u 7.239s <e>0:23.10</e> 128.0%   0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST# time tar --listed-incremental=/dev/null -xf ../diff.tar.gz
	22.574u 7.308s <e>0:23.61</e> 126.5%   0+0k 1288+0io 7pf+0w
	devuan:/mnt/memdisk/DST# diff -s ../SRC/debian-10.6.0-amd64-DVD-2.iso SRC/debian-10.6.0-amd64-DVD-2.iso
	Files ../SRC/debian-10.6.0-amd64-DVD-2.iso and SRC/debian-10.6.0-amd64-DVD-2.iso are identical
	devuan:/mnt/memdisk/DST#
    </code></div>

    <p>
      For <i>tar</i> the operation took:
    </p>

    <ul>
      <li>131.08 seconds for full backup</li>
      <li>131.16 seconds for differential backup</li>
      <li>4.3 GiB for full backup (compression ratio of 0,22 %)</li>
      <li>4.3 GiB for the differential backup (compression ratio of 0,22 %)</li>
      <li>23,10 seconds to restore the full backup</li>
      <li>23,61 seconds to restore the differential backup</li>
      <li>154,18 seconds for the full backup+restoration</li>
      <li>154,77 seconds for the differential backup+restoration</li>
    </ul>
    <br/>

    <div><code>
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time rsync -arvHAXqz SRC/* DST
	156.844u 8.501s <e>2:10.62</e> 126.5%  0+0k 1144+0io 5pf+0w
	devuan:/mnt/memdisk# ./bitflip 100000 SRC/debian-10.6.0-amd64-DVD-2.iso
	devuan:/mnt/memdisk# time rsync -arvHAXqz SRC/* DST
	166.080u 8.610s <e>2:20.11</e> 124.6%  0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# diff -s SRC/debian-10.6.0-amd64-DVD-2.iso DST/debian-10.6.0-amd64-DVD-2.iso
	Files SRC/debian-10.6.0-amd64-DVD-2.iso and DST are identical
	devuan:/mnt/memdisk#
    </code></div>

    <p>
      For <i>rsync</i> the operation took:
    </p>
    <ul>
      <li>130.62 seconds for full backup+restoration</li>
      <li>140.11 seconds for differential backup+restoration</li>
      <li>4.3 GiB for the full backup (compression ratio 0%)</li>
      <li>0 GiB for the differential backup (it has replaced the full backup)</li>
    </ul>

    <p>
      <u>In conclusion:</u> The storage requirement for <i>rsync</i> and <i>dar</i> is the best,
      with the advantage for <i>dar</i> to provide backup history, or the ability to restore a
      file that has been removed by mistake even long ago, or a whole system in a previous state. But
      in term of execution time,
      <i>dar</i> is the slowest of all for the full backup, but the fastest and far before the others
      for any further differential backup. Thus, unless you make just a single incremental or
      differential backup after each full backup, <i>dar</i> surpasses both <i>tar</i>
      and <i>rsync</i> in both storage requirement, network usage, transfer time and
      execution time.
    </p>

    <h5>Added and removed files</h5>

    <p>
      The test here is to make a full backup of the linux source tree, then
      rename the <i>Documentation</i> directory as <i>doc</i> and make a
      differential backup on it. Renaming moves files, but for most software
      this is the removal of a directory and the creation of a new one.
    </p>
    <div><code>
	devuan:/mnt/memdisk# du -B1 -s SRC
	<e>1121144832</e>      SRC
	devuan:/mnt/memdisk# time dar -c full -R SRC -z6 -q
	23.733u 1.236s <e>0:24.98</e> 99.9%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -c full2 -R SRC -z6 -q --delta sig
	25.030u 1.356s <e>0:26.40</e> 99.9%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -c full-at-1-0 -R SRC -z6 -q -at -1 0
	21.746u 1.112s <e>0:22.87</e> 99.9%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# cd SRC/linux-5.9.2/
	devuan:/mnt/memdisk/SRC/linux-5.9.2# mv Documentation/ doc
	devuan:/mnt/memdisk/SRC/linux-5.9.2# cd ../..
	devuan:/mnt/memdisk# time dar -c diff -A full -R SRC -z6 -q
	2.992u 0.522s <e>0:03.53</e> 99.4%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -c diff-at-1-0 -A full-at-1-0 -R SRC -z6 -q -at -1 0
	2.784u 0.419s <e>0:03.21</e> 99.3%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ls -l *.dar
	-rw-r--r-- 1 root root  <e>12184277</e> Nov  1 18:25 diff-at-1-0.1.dar
	-rw-r--r-- 1 root root  <e>17858927</e> Nov  1 18:18 diff.1.dar
	-rw-r--r-- 1 root root <e>212368008</e> Nov  1 18:16 full-at-1-0.1.dar
	-rw-r--r-- 1 root root <e>219047658</e> Nov  1 18:14 full.1.dar
	-rw-r--r-- 1 root root <e>225070952</e> Nov  1 18:15 full2.1.dar
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time dar -x full -R DST -q
	4.998u 1.151s <e>0:06.16</e> 99.6%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -x diff -R DST -q -w
	1.507u 0.539s <e>0:02.06</e> 98.5%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# diff -r SRC DST && echo "same data" || echo "different data"
	same data
	devuan:/mnt/memdisk#rm -rf DST ; mkdir DST
	devuan:/mnt/memdisk# time dar -x full-at-1-0 -R DST -q
	5.107u 1.863s <e>0:06.98</e> 99.7%     0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# time dar -x diff-at-1-0 -R DST -q -w
	1.434u 0.527s <e>0:01.97</e> 98.9%     0+0k 0+0io 0pf+0w
    </code></div>
    <p>
      Above we did a full backup without <i>--delta sig</i> because
      it does not worth calculating delta signature for each file as these
      are a lot of small files. But just for checking, we did a second backup
      <i>full2</i> with delta signatures. The execution is slightly longer
      (5 % more) and the resulting backup is 2.7 % larger, which does not
      make a big difference. We also checked the <u>fast mode</u> which brings
      a slight better performance.
    </p>
      for <i>dar</i> the operation took:
    </p>
    <ul>
      <li>24.98 seconds for full backup (22,87 in <u>fast mode</u>)</li>
      <li>3.53 seconds for differential backup (3,21 in <u>fast mode</u>)</li>
      <li>208 MiB for the full backup (202 MiB in <u>fast mode</u>)</li>
      <li>17.03 MiB for the differential backup (11,6 MiB in <u>fast mode</u>)</li>
      <li>6.16 seconds to restore the full backup (6.98 in <u>fast mode</u>)</li>
      <li>2.06 second to restore the differential backup (1.97 in <u>fast mode</u>)</li>
      <li>making a total of 31,14 seconds for the full backup+restoration (29,85 in <u>fast mode</u>)</li>
      <li>and a total of 5,59 seconds for the differential backup+restoration (5,18 in <u>fast mode</u>)</li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# time tar --listed-incremental=snapshot.file -czf full.tar.gz SRC
	25.452u 2.301s <e>0:24.35</e> 113.9%   0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# cd SRC/linux-5.9.2/
	devuan:/mnt/memdisk/SRC/linux-5.9.2# mv Documentation/ doc
	devuan:/mnt/memdisk/SRC/linux-5.9.2# cd ../..
	devuan:/mnt/memdisk#  time tar --listed-incremental=snapshot.file -czf diff.tar.gz SRC
	1.750u 0.260s <e>0:01.71</e> 117.5%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# ls -l
	total 190488
	drwxr-xr-x 3 root root        60 Oct 31 19:37 SRC
	-rw-r--r-- 1 root root   <e>9654445</e> Oct 31 19:49 diff.tar.gz
	-rw-r--r-- 1 root root <e>184036391</e> Oct 31 19:49 full.tar.gz
	-rw-r--r-- 1 root root   <e>1361962</e> Oct 31 19:49 snapshot.file
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# cd DST
	devuan:/mnt/memdisk/DST# time tar --listed-incremental=/dev/null -xf ../full.tar.gz
	5.272u 1.709s <e>0:04.98</e> 139.9%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST# time tar --listed-incremental=/dev/null -xf ../diff.tar.gz
	0.521u 0.343s <e>0:00.62</e> 138.7%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk/DST# cd ..
	devuan:/mnt/memdisk# diff -r SRC DST/SRC && echo "same data" || echo "different data"
	same data
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      For <i>tar</i> the operation took:
    </p>
    <ul>
      <li>24.35 seconds for the full backup</li>
      <li>1.71 second for the differential backup</li>
      <li>176 MiB for the full backup (full.tar.gz + snapshot.file)</li>
      <li>9,2 MiB for the differential backup</li>
      <li>4.98 seconds for the restoration of the full backup</li>
      <li>0.62 second for the restoration of the differential backup</li>
      <li>making a total of 29.33 seconds for the full backup+restoration</li>
      <li>and a total of 2.33 seconds for the differential backup+restoration</li>
    </ul>
    <br/>
    <div><code>
	devuan:/mnt/memdisk# mkdir DST
	devuan:/mnt/memdisk# time rsync -arvHAXqz --delete SRC/* DST
	30.647u 3.429s 0:22.62 150.5%   0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# cd SRC/linux-5.9.2/
	devuan:/mnt/memdisk/SRC/linux-5.9.2# mv Documentation/ doc
	devuan:/mnt/memdisk/SRC/linux-5.9.2# cd ../..
	devuan:/mnt/memdisk# time rsync -arvHAXqz --delete SRC/* DST
	2.392u 1.191s 0:02.86 125.1%    0+0k 0+0io 0pf+0w
	devuan:/mnt/memdisk# diff -r SRC DST && echo "same data" || echo "different data"
	same data
	devuan:/mnt/memdisk#
    </code></div>
    <p>
      For <i>rsync</i> the operation took:
    </p>
    <ul>
      <li>22,62 seconds for the full backup+restoration</li>
      <li>2,86 seconds for the differential backup+restoration</li>
      <li>1069 MiB for the backup size (no compression)</li>
    </ul>
    <p>
      <u>In conclusion</u> For backup operation <i>dar</i> is faster that <i>tar</i> but provides
      less good compression ratio: compression is performed per file, not globally, which let <i>dar</i>
      recover any file passed a corrupted one, and allow quick restoration of a single file among a
      larged compressed file, this is the cost of the robustness compared to <i>tar</i>.
      But overall, <i>tar</i> is not much faster than <i>dar</i> to justify a non exhaustive backup (rich
      metadata and sparse are ignored by <i>tar</i>). If backup history is expected <i>dar</i> is a good
      choice, else <i>rsync</i> is better adapted.
    <br/>
    <br/>
    <br/>
    <br/>
    <br/>
  </body>
</html>
